{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "image_seg_def.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnZ43BS8EbNk",
        "colab_type": "text"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "TvMiNlEIEbNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "42f9c637-9dff-4f1a-fda2-31d2eb0c85b8"
      },
      "source": [
        "!pip install tensorflow-datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (4.28.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (3.10.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (19.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.21.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.11.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.3.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.17.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets) (41.4.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2019.9.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "o60kb2L0EbN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "114d6241-2632-4551-89ae-5ff0670bb831"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# import larq as lq\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOU3dDFnEbN8",
        "colab_type": "text"
      },
      "source": [
        "> # Get Oxford pet dataset dataset from tf datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q1d7YpkzEbN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, info_1 = tfds.load(name=\"oxford_iiit_pet:3.0.0\", split=\"train\",with_info=True) \n",
        "test_data, info_2 = tfds.load(name=\"oxford_iiit_pet:3.0.0\", split=\"test\",with_info=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-q6y00nREbOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_data(data_val):\n",
        "    \n",
        "    image = data_val['image']\n",
        "    mask = data_val['segmentation_mask']\n",
        "    image = tf.image.resize(image, (128, 128))\n",
        "    mask = tf.image.resize(mask, (128, 128))\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    # Normalize the pixel values\n",
        "    image = image / 255.0\n",
        "    # Change mask vals to [0,1,2]\n",
        "    mask -= 1\n",
        "    return image, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fptAxYMrEbON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_augmentation(data_val):\n",
        "    \n",
        "    #Normalize both training and testing images\n",
        "   \n",
        "    image, mask = normalize_data(data_val)\n",
        "    #Augmentation only for training\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        #Apply to both image and maska\n",
        "        image = tf.image.flip_left_right(image)\n",
        "        mask = tf.image.flip_left_right(mask)\n",
        "  \n",
        "    \n",
        "    return image, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QTDJ_IWNEbOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9f184aa5-4b8f-4b2f-a8a0-7549611c38d7"
      },
      "source": [
        "print(\"Training samples: {} \\nTest samples: {} \".format(info_1.splits['train'].num_examples,info_2.splits['test'].num_examples))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples: 3680 \n",
            "Test samples: 3669 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MQc231wMEbOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train_data.map(data_augmentation, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test = test_data.map(normalize_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "saLrGMJNEbOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = info_1.splits['train'].num_examples\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l1I7rlsAEbOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = train.cache().shuffle(1000).batch(batch_size).repeat()\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VEs-7vefEbOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(n_filters,X,reduce = True):\n",
        "    \n",
        "   \n",
        "    Y = Conv2D(n_filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(X)\n",
        "    Y = BatchNormalization()(Y)\n",
        "    Y = Conv2D(n_filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(Y)\n",
        "    Y = BatchNormalization()(Y)\n",
        "    if reduce:\n",
        "        Y = MaxPool2D(pool_size=(2,2),strides=(2,2))(Y)\n",
        "    \n",
        "    return Y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qRmQvGIdEbO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All quantized layers except the first will use the same options\n",
        "\n",
        "def create_model():\n",
        "    \n",
        "    inputs = tf.keras.Input(shape=(128,128,3))\n",
        "    \n",
        "    conv0 = conv_block(32,inputs,reduce=False)\n",
        "       \n",
        "    conv1 = conv_block(64,conv0)\n",
        "    \n",
        "    conv2 = conv_block(128,conv1)\n",
        "    \n",
        "    conv3 = conv_block(256,conv2)\n",
        "    \n",
        "    conv4 = conv_block(512,conv3)\n",
        "    \n",
        "    up1 = concatenate([UpSampling2D()(conv4),conv3],axis=3)\n",
        "    up1 = conv_block(256,up1,reduce=False)\n",
        "    \n",
        "    up2 = concatenate([UpSampling2D()(up1),conv2],axis=3)\n",
        "    up2 = conv_block(128,up2,reduce=False)\n",
        "    \n",
        "    up3 = concatenate([UpSampling2D()(up2),conv1],axis=3)\n",
        "    up3 = conv_block(64,up3,reduce=False)\n",
        "    \n",
        "    up4 = concatenate([UpSampling2D()(up3),conv0],axis=3)\n",
        "    up4 = conv_block(32,up4,reduce=False)\n",
        "    \n",
        "    l_out = Conv2D(3, (1, 1))(up4)\n",
        "    outputs = Activation(\"softmax\")(l_out)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=[inputs],outputs=[outputs])\n",
        "    \n",
        "    return model\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3Zr8_4VIEbO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    return 1e-3 * 0.1 ** (epoch // 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KVBDfZPZEbPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model():\n",
        "    model = create_model()\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    logs = keras.callbacks.CSVLogger('seg_def.log')\n",
        "    callbacks = [logs]\n",
        "#     if not bop:\n",
        "#         scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "#         callbacks.append(scheduler)\n",
        "    \n",
        "    trained_model = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=300,\n",
        "    steps_per_epoch= info_1.splits['train'].num_examples // batch_size,\n",
        "    validation_data= test_dataset,\n",
        "    validation_steps= info_1.splits['test'].num_examples // batch_size,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    model.save('seg_def.h5')\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S6ojlYb8EbPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab5ff91d-829a-4b5a-8db6-1611e5d1d982"
      },
      "source": [
        "# # For baseline set bop to false\n",
        "train_model()\n",
        "\"\"\"\n",
        "The results of the model are not tested as they produce very poor segmentation\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe results of the model are not tested as they produce very poor segmentation\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}