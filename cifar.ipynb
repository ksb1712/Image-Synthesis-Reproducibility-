{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "cifar_model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "XsMEw05L8G-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install larq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "xagdG0JX8G-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import larq as lq\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uscNwXKv8G-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get Cifar10 from tf datasets\n",
        "trainData, testData = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4RCgZt-X8G-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Augmentation\n",
        "def resize_and_flip(image, labels, training):\n",
        "    #Normalize\n",
        "    image = tf.cast(image, tf.float32) / (255./2.) - 1. \n",
        "    if training:\n",
        "        #Padding\n",
        "        image = tf.image.resize_with_crop_or_pad(image, 40, 40)\n",
        "        #crop\n",
        "        image = tf.image.random_crop(image, [32, 32, 3])\n",
        "        #flip\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "    return image, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0CIvmzEC8G_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate batches of data\n",
        "def create_dataset(data, batch_size, training):\n",
        "    images, labels = data\n",
        "    labels = tf.one_hot(np.squeeze(labels), 10)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    dataset = dataset.repeat()\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.map(lambda x, y: resize_and_flip(x, y, training))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RdOkS2xR8G_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "train_dataset = create_dataset(trainData, batch_size, True)\n",
        "test_dataset = create_dataset(testData, batch_size, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "juRA7mYY8G_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Derived from example at https://larq.dev/\n",
        "\n",
        "def create_model(model_params):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        lq.layers.QuantConv2D(128, 3,\n",
        "                              kernel_quantizer=\"ste_sign\",\n",
        "                              kernel_constraint=\"weight_clip\",\n",
        "                              use_bias=False,\n",
        "                              input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(128, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(256, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(256, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(512, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(512, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "        tf.keras.layers.Flatten(),\n",
        "\n",
        "        lq.layers.QuantDense(1024, **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantDense(1024, **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantDense(10, **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "        tf.keras.layers.Activation(\"softmax\")\n",
        "    ])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tafkbbvl8G_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_params = dict(input_quantizer=\"ste_sign\",\n",
        "              kernel_quantizer= tf.keras.layers.Activation(\"linear\"),\n",
        "              kernel_constraint= None,\n",
        "              use_bias=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fdAyuui28G_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Return suitable optimizer bop or adam\n",
        "def optimizer_fn(bop=True):\n",
        "    if bop:\n",
        "        initial_lr = 1e-2\n",
        "    else:\n",
        "        initial_lr = 1e-3\n",
        "    threshold_val = 1e-8\n",
        "    gamma_val = 1e-4\n",
        "    gamma_decay = 0.1\n",
        "    decay_step = int((50000 / 50) * 100)\n",
        "    adam = tf.keras.optimizers.Adam(lr=initial_lr)\n",
        "    \n",
        "    if bop: \n",
        "        optimizer=lq.optimizers.Bop(fp_optimizer=adam, threshold= threshold_val, gamma=tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            gamma_val, decay_step, gamma_decay, staircase=True))\n",
        "        return optimizer\n",
        "    \n",
        "    else:\n",
        "        return adam\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eyTdBUHK8G_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    return 1e-3 * 0.1 ** (epoch // 100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "C0CAH_U08G_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(bop=True):\n",
        "    model = create_model(model_params)\n",
        "    model.compile(\n",
        "        optimizer=optimizer_fn(bop),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    logs = keras.callbacks.CSVLogger('cifar.log')\n",
        "    callbacks = [logs]\n",
        "    if not bop:\n",
        "        scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "        callbacks.append(scheduler)\n",
        "    \n",
        "    trained_model = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=500,\n",
        "    steps_per_epoch= trainData[1].shape[0] // batch_size,\n",
        "    validation_data=test_dataset,\n",
        "    validation_steps=testData[1].shape[0] // batch_size,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        "    )\n",
        "    if bop:\n",
        "        model.save('cifar_bop.h5')\n",
        "    else:\n",
        "        model.save('cifar_baseline.h5')\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "M7cpN1az8G_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(bop=True):\n",
        "    model = create_model(model_params)\n",
        "    model.compile(\n",
        "        optimizer=optimizer_fn(bop),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    if bop:\n",
        "        model.load_weights('cifar_bop.h5')\n",
        "    else:\n",
        "        model.load_weights('cifar_baseline.h5')\n",
        "    y_new = np_utils.to_categorical(testData[1])\n",
        "    scores = model.evaluate(testData[0],y_new, verbose=1)\n",
        "    print(\"Accuracy of model is {}\".format(scores[1]*100.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i-cxR-KZ8G_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For baseline set bop to false\n",
        "train_model(bop=True)\n",
        "test_model(bop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB5s55ff8R47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}