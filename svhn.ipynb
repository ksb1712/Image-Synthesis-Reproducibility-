{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "svhn_model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P-w_ZSW3kAI",
        "colab_type": "text"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "IhO2BLiC3kAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install larq tensorflow-datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "L2SnCOxo3kAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import larq as lq\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujwLUiy23kAV",
        "colab_type": "text"
      },
      "source": [
        "# Get SVHN dataset from tf datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3GqoO__G3kAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tfds.load(name=\"svhn_cropped\", split=\"train\", batch_size=-1 ) \n",
        "test_data = tfds.load(name=\"svhn_cropped\", split=\"test\", batch_size=-1)\n",
        "val_data = tfds.load(name=\"svhn_cropped\", split = \"extra\", batch_size=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5lXg46yM3kAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to numpy\n",
        "train = tfds.as_numpy(train_data)\n",
        "test = tfds.as_numpy(test_data)\n",
        "val = tfds.as_numpy(val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oAlFxDqx3kAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split x and y\n",
        "x_train, y_train = train[\"image\"], train[\"label\"]\n",
        "x_test, y_test = test[\"image\"], test[\"label\"]\n",
        "x_val, y_val = val[\"image\"], val[\"label\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MH9S83G-3kAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reduce val set\n",
        "x_val, y_val = x_val[:30000,:,:,:], y_val[:30000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cnC1aMVm3kAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Training samples: {} \\nTest samples: {} \\nVal samples: {}\".format(len(x_train),len(x_test),len(x_val)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AtZz572t3kAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Augmentation\n",
        "def resize_and_flip(image, labels, training):\n",
        "    #Normalize\n",
        "    image = tf.cast(image, tf.float32) / (255./2.) - 1. \n",
        "    if training:\n",
        "        #Padding\n",
        "        image = tf.image.resize_with_crop_or_pad(image, 40, 40)\n",
        "        #crop\n",
        "        image = tf.image.random_crop(image, [32, 32, 3])\n",
        "        #flip\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "    return image, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SQImqa9j3kAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dataset creation (batch generation)\n",
        "def create_dataset(images,labels, batch_size, training):\n",
        "    labels = tf.one_hot(np.squeeze(labels), 10)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "    dataset = dataset.repeat()\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.map(lambda x, y: resize_and_flip(x, y, training))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v64df4dF3kAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "train_dataset = create_dataset(x_train,y_train, batch_size, True)\n",
        "val_dataset = create_dataset(x_val,y_val, batch_size, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWT674EK3kAs",
        "colab_type": "text"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vIa4kZse3kAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Derived from example at https://larq.dev/\n",
        "\n",
        "def create_model(model_params):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        lq.layers.QuantConv2D(128, 3,\n",
        "                              kernel_quantizer=\"ste_sign\",\n",
        "                              kernel_constraint=\"weight_clip\",\n",
        "                              use_bias=False,\n",
        "                              input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(128, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(256, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(256, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(512, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantConv2D(512, 3, padding=\"same\", **model_params),\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "        tf.keras.layers.Flatten(),\n",
        "\n",
        "        lq.layers.QuantDense(1024, **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantDense(1024, **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "\n",
        "        lq.layers.QuantDense(10, **model_params),\n",
        "        tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
        "        tf.keras.layers.Activation(\"softmax\")\n",
        "    ])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VOoAeVbX3kAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_params = dict(input_quantizer=\"ste_sign\",\n",
        "              kernel_quantizer= tf.keras.layers.Activation(\"linear\"),\n",
        "              kernel_constraint= None,\n",
        "              use_bias=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6no6cpgJ3kAz",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b52NKrlZ3kAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Return suitable optimizer bop or adam\n",
        "def optimizer_fn(bop=True):\n",
        "    if bop:\n",
        "        initial_lr = 1e-2\n",
        "    else:\n",
        "        initial_lr = 1e-3\n",
        "    threshold_val = 1e-8\n",
        "    gamma_val = 1e-4\n",
        "    gamma_decay = 0.1\n",
        "    decay_step = int((50000 / 50) * 100)\n",
        "    adam = tf.keras.optimizers.Adam(lr=initial_lr)\n",
        "    \n",
        "    if bop: \n",
        "        optimizer=lq.optimizers.Bop(fp_optimizer=adam, threshold= threshold_val, gamma=tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            gamma_val, decay_step, gamma_decay, staircase=True))\n",
        "        return optimizer\n",
        "    \n",
        "    else:\n",
        "        return adam\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BG2nTgCB3kA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    return 1e-3 * 0.1 ** (epoch // 100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdSSyc7K3kA4",
        "colab_type": "text"
      },
      "source": [
        "# Train model and save "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9F1lGKEG3kA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(bop=True):\n",
        "    model = create_model(model_params)\n",
        "    model.compile(\n",
        "        optimizer=optimizer_fn(bop),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    logs = keras.callbacks.CSVLogger('svhn.log')\n",
        "    callbacks = [logs]\n",
        "    if not bop:\n",
        "        scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "        callbacks.append(scheduler)\n",
        "    \n",
        "    trained_model = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=300,\n",
        "    steps_per_epoch= len(x_train) // batch_size,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps= len(x_val) // batch_size,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        "    )\n",
        "    if bop:\n",
        "        model.save('svhn_bop.h5')\n",
        "    else:\n",
        "        model.save('svhn_baseline.h5')\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OBWoOVQa3kA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(bop=True):\n",
        "    model = create_model(model_params)\n",
        "    model.compile(\n",
        "        optimizer=optimizer_fn(bop),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    if bop:\n",
        "        model.load_weights('svhn_bop.h5')\n",
        "    else:\n",
        "        model.load_weights('svhn_baseline.h5')\n",
        "    y_new = np_utils.to_categorical(y_test)\n",
        "    scores = model.evaluate(x_test, y_new, verbose=1)\n",
        "    print(\"Accuracy of model is {}\".format(scores[1]*100.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npcH-ayS66dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For baseline set bop to false\n",
        "train_model(bop=True)\n",
        "test_model(bop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjPJalTi7NH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}